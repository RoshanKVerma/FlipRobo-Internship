{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "456e0cd3",
   "metadata": {},
   "source": [
    "# Assignment on Web-Scrapping using Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7b14058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imoprting required libraries needed for our requirements\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException , NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fdd101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7580601e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbcf3dfd",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "    have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "    jobs data.\n",
    "    This task will be done in following steps:\n",
    "        1. First get the webpage https://www.naukri.com/\n",
    "        2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "            location” field.\n",
    "        3. Then click the search button.\n",
    "        4. Then scrape the data for the first 10 jobs results you get.\n",
    "        5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b7f15e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experience_Requuired</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Bayer</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Optum</td>\n",
       "      <td>5-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reference Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Cargill</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst, plsql, Tableau, Informatica</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Hyderabad/Secunder...</td>\n",
       "      <td>Capgemini</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Analyst - SQL/Tableau</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Global Employees</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Analyst II</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst - Data Visualization &amp; Reporting</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Hyderabad/Secund...</td>\n",
       "      <td>Global Employees</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Bright Money</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Leap COE Intern(Data Analyst)</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Info Origin Inc.</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Job_Title  \\\n",
       "0                                   Data Analyst   \n",
       "1                            Senior Data Analyst   \n",
       "2                         Reference Data Analyst   \n",
       "3                                   Data Analyst   \n",
       "4      Data Analyst, plsql, Tableau, Informatica   \n",
       "5              Senior Data Analyst - SQL/Tableau   \n",
       "6                         Senior Data Analyst II   \n",
       "7  Data Analyst - Data Visualization & Reporting   \n",
       "8                                   Data Analyst   \n",
       "9                  Leap COE Intern(Data Analyst)   \n",
       "\n",
       "                                        Job_Location      Company_Name  \\\n",
       "0                                Bangalore/Bengaluru             Bayer   \n",
       "1                                Bangalore/Bengaluru             Optum   \n",
       "2                                Bangalore/Bengaluru     Deutsche Bank   \n",
       "3                                Bangalore/Bengaluru           Cargill   \n",
       "4  Bangalore/Bengaluru, Noida, Hyderabad/Secunder...         Capgemini   \n",
       "5  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...  Global Employees   \n",
       "6                                Bangalore/Bengaluru          Flipkart   \n",
       "7  Bangalore/Bengaluru, Kolkata, Hyderabad/Secund...  Global Employees   \n",
       "8                                Bangalore/Bengaluru      Bright Money   \n",
       "9                                Bangalore/Bengaluru  Info Origin Inc.   \n",
       "\n",
       "  Experience_Requuired  \n",
       "0              2-5 Yrs  \n",
       "1              5-7 Yrs  \n",
       "2              2-5 Yrs  \n",
       "3              3-5 Yrs  \n",
       "4              3-8 Yrs  \n",
       "5             7-12 Yrs  \n",
       "6              3-6 Yrs  \n",
       "7             5-10 Yrs  \n",
       "8              0-2 Yrs  \n",
       "9              0-1 Yrs  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an instance of webdriver for google chrome\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\RoshanV\\Desktop\\Internship\\Assignment 2\\chromedriver.exe\")\n",
    "# Using webdriver we'll now open the website in chrome\n",
    "driver.get(\"http://www.naukri.com\")\n",
    "\n",
    "designation=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[1]/div/div/div/input\")\n",
    "designation.send_keys('Data Analyst')\n",
    "# Find using xpath\n",
    "location=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input')\n",
    "location.send_keys('Bangalore')\n",
    "# Find using xpath\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[3]/div/div/div[6]')\n",
    "search.click()\n",
    "\n",
    "# Making empty list for our data to be dumped\n",
    "JobDesignation=[]\n",
    "JobLoc=[]\n",
    "CompanyName=[]\n",
    "Required_Exp=[]\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "\n",
    "# Extracting Data from The Website using XPATH and Dumping it in respective Lists\n",
    "\n",
    "jtt=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in jtt[0:10]:\n",
    "    title=i.text\n",
    "    JobDesignation.append(title)\n",
    "    \n",
    "jlt=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in jlt[0:10]:\n",
    "    location=i.text\n",
    "    JobLoc.append(location)\n",
    "    \n",
    "cnt=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in cnt[0:10]:\n",
    "    compname=i.text\n",
    "    CompanyName.append(compname)\n",
    "    \n",
    "ret=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]//span')\n",
    "for i in ret[0:10]:\n",
    "    exp=i.text\n",
    "    Required_Exp.append(exp)\n",
    "    \n",
    "# Using Pandas to Create DataFrame of the various Lists\n",
    "df=pd.DataFrame({'Job_Title':JobDesignation,'Job_Location':JobLoc,'Company_Name':CompanyName,'Experience_Requuired':Required_Exp})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab55450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00c161e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "587a7804",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "    have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "    This task will be done in following steps:\n",
    "        1. First get the webpage https://www.naukri.com/\n",
    "        2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "            location” field.\n",
    "        3. Then click the search button.\n",
    "        4. Then scrape the data for the first 10 jobs results you get.\n",
    "        5. Finally create a dataframe of the scraped data.\n",
    "    Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e438a235",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experience_Requuired</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Manager - EmTech - Machine Learning - P&amp;T</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...</td>\n",
       "      <td>PwC</td>\n",
       "      <td>5-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Assistant Manager - Data Science</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune</td>\n",
       "      <td>CitiusTech</td>\n",
       "      <td>5-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data scientist _Tata Consultancy Services(Tcs)</td>\n",
       "      <td>Bangalore/Bengaluru, Kochi/Cochin, Indore, New...</td>\n",
       "      <td>TATA CONSULTANCY SERVICES (TCS)</td>\n",
       "      <td>9-14 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science Senior Analyst</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Job||Job Opening For AI Technologist - Data Sc...</td>\n",
       "      <td>Bangalore/Bengaluru, New Delhi, Hyderabad/Secu...</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hiring For DATA Scientist @ NTT DATA Business ...</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Hyderabad/Secunder...</td>\n",
       "      <td>NTT DATA Business Solutions Private Limited</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Urgent Job Opening For AI Practitioner - Data ...</td>\n",
       "      <td>Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>11-20 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Expert Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai</td>\n",
       "      <td>United Phosphorus Limited</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BCAI - Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Bosch Global Software Technologies</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0   Senior Manager - EmTech - Machine Learning - P&T   \n",
       "1                   Assistant Manager - Data Science   \n",
       "2                   Analystics & Modeling Specialist   \n",
       "3     Data scientist _Tata Consultancy Services(Tcs)   \n",
       "4                        Data Science Senior Analyst   \n",
       "5  Job||Job Opening For AI Technologist - Data Sc...   \n",
       "6  Hiring For DATA Scientist @ NTT DATA Business ...   \n",
       "7  Urgent Job Opening For AI Practitioner - Data ...   \n",
       "8                              Expert Data Scientist   \n",
       "9                       BCAI - Senior Data Scientist   \n",
       "\n",
       "                                        Job_Location  \\\n",
       "0  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...   \n",
       "1                  Bangalore/Bengaluru, Mumbai, Pune   \n",
       "2  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...   \n",
       "3  Bangalore/Bengaluru, Kochi/Cochin, Indore, New...   \n",
       "4  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...   \n",
       "5  Bangalore/Bengaluru, New Delhi, Hyderabad/Secu...   \n",
       "6  Bangalore/Bengaluru, Noida, Hyderabad/Secunder...   \n",
       "7  Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...   \n",
       "8                        Bangalore/Bengaluru, Mumbai   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                                  Company_Name Experience_Requuired  \n",
       "0                                          PwC             5-12 Yrs  \n",
       "1                                   CitiusTech              5-9 Yrs  \n",
       "2                                    Accenture              6-8 Yrs  \n",
       "3              TATA CONSULTANCY SERVICES (TCS)             9-14 Yrs  \n",
       "4                                    Accenture              2-5 Yrs  \n",
       "5                                        Wipro             5-10 Yrs  \n",
       "6  NTT DATA Business Solutions Private Limited              4-9 Yrs  \n",
       "7                                        Wipro            11-20 Yrs  \n",
       "8                    United Phosphorus Limited             6-11 Yrs  \n",
       "9           Bosch Global Software Technologies              5-8 Yrs  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Creating an instance of webdriver for google chrome\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\RoshanV\\Desktop\\Internship\\Assignment 2\\chromedriver.exe\")\n",
    "# Using webdriver we'll now open the website in chrome\n",
    "driver.get(\"http://www.naukri.com\")\n",
    "# Find using xpath\n",
    "designation=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[1]/div/div/div/input\")\n",
    "designation.send_keys('Data Scientist') # Typing Data Scientist\n",
    "# Find using xpath\n",
    "location=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input')\n",
    "location.send_keys('Bangalore') # Typing Banglore\n",
    "\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[3]/div/div/div[6]')\n",
    "search.click()\n",
    "\n",
    "# Making empty list for our data to be dumped\n",
    "JobDesignation=[]\n",
    "JobLoc=[]\n",
    "CompanyName=[]\n",
    "Required_Exp=[]\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "# Extracting Data from The Website using XPATH and Dumping it in respective Lists\n",
    "\n",
    "jtt=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in jtt[0:10]:\n",
    "    title=i.text\n",
    "    JobDesignation.append(title)\n",
    "    \n",
    "jlt=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in jlt[0:10]:\n",
    "    location=i.text\n",
    "    JobLoc.append(location)\n",
    "    \n",
    "cnt=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in cnt[0:10]:\n",
    "    compname=i.text\n",
    "    CompanyName.append(compname)\n",
    "    \n",
    "ret=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]//span')\n",
    "for i in ret[0:10]:\n",
    "    exp=i.text\n",
    "    Required_Exp.append(exp)\n",
    "    \n",
    "\n",
    "# Using Pandas to Create DataFrame of the various Lists\n",
    "df=pd.DataFrame({'Job_Title':JobDesignation,'Job_Location':JobLoc,'Company_Name':CompanyName,'Experience_Requuired':Required_Exp})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292819e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ef8696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13215ed0",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "    You have to use the location and salary filter.\n",
    "    You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "    You have to scrape the job-title, job-location, company name, experience required.\n",
    "    The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "    The task will be done as shown in the below steps:\n",
    "        1. first get the webpage https://www.naukri.com/\n",
    "        2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "        3. Then click the search button.\n",
    "        4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "        5. Then scrape the data for the first 10 jobs results you get.\n",
    "        6. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2467b991",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experience_Requuired</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Job||Job Opening For AI Technologist - Data Sc...</td>\n",
       "      <td>New Delhi, Hyderabad/Secunderabad, Pune, Chenn...</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist - Noida/Bangalore</td>\n",
       "      <td>Noida, Bangalore/Bengaluru</td>\n",
       "      <td>EXL</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DigitalBCG GAMMA Data Scientist</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Optum</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chat-bot Developer / Data Scientist</td>\n",
       "      <td>Mumbai, New Delhi, Bangalore/Bengaluru\\n(WFH d...</td>\n",
       "      <td>Big Seo Buzz</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist / Chat-bot Developer</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru, Mumbai (All Ar...</td>\n",
       "      <td>Big Seo Buzz</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - Engine Algorithm</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>Primo Hiring</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist For Healthcare Product team</td>\n",
       "      <td>Delhi / NCR, Chennai, Bangalore/Bengaluru</td>\n",
       "      <td>SECUREKLOUD TECHNOLOGIES</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida, Gurgaon/Gurugram, Jaipur</td>\n",
       "      <td>Celebal Technologies</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Webhelp India Private Limited</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0  Job||Job Opening For AI Technologist - Data Sc...   \n",
       "1                   Data Scientist - Noida/Bangalore   \n",
       "2                    DigitalBCG GAMMA Data Scientist   \n",
       "3                                     Data Scientist   \n",
       "4                Chat-bot Developer / Data Scientist   \n",
       "5                Data Scientist / Chat-bot Developer   \n",
       "6                  Data Scientist - Engine Algorithm   \n",
       "7         Data Scientist For Healthcare Product team   \n",
       "8                                     Data Scientist   \n",
       "9                                     Data Scientist   \n",
       "\n",
       "                                        Job_Location  \\\n",
       "0  New Delhi, Hyderabad/Secunderabad, Pune, Chenn...   \n",
       "1                         Noida, Bangalore/Bengaluru   \n",
       "2                     New Delhi, Bangalore/Bengaluru   \n",
       "3                                   Gurgaon/Gurugram   \n",
       "4  Mumbai, New Delhi, Bangalore/Bengaluru\\n(WFH d...   \n",
       "5  New Delhi, Bangalore/Bengaluru, Mumbai (All Ar...   \n",
       "6  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...   \n",
       "7          Delhi / NCR, Chennai, Bangalore/Bengaluru   \n",
       "8                    Noida, Gurgaon/Gurugram, Jaipur   \n",
       "9                                   Gurgaon/Gurugram   \n",
       "\n",
       "                    Company_Name Experience_Requuired  \n",
       "0                          Wipro             5-10 Yrs  \n",
       "1                            EXL             5-10 Yrs  \n",
       "2        Boston Consulting Group              2-5 Yrs  \n",
       "3                          Optum              2-7 Yrs  \n",
       "4                   Big Seo Buzz              2-7 Yrs  \n",
       "5                   Big Seo Buzz              3-7 Yrs  \n",
       "6                   Primo Hiring              1-3 Yrs  \n",
       "7       SECUREKLOUD TECHNOLOGIES              2-7 Yrs  \n",
       "8           Celebal Technologies              1-5 Yrs  \n",
       "9  Webhelp India Private Limited              2-7 Yrs  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Creating an instance of webdriver for google chrome\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\RoshanV\\Desktop\\Internship\\Assignment 2\\chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "# Using webdriver we'll now open the website in chrome\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "driver.implicitly_wait(10)\n",
    "# Find using xpath\n",
    "designation=driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[1]/div/div/div/input\")\n",
    "designation.send_keys('Data Scientist')\n",
    "# Find using xpath\n",
    "search=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[3]/div/div/div[6]')\n",
    "search.click()\n",
    "\n",
    "driver.find_element(By.XPATH,\"//span[text()='Delhi / NCR']\").click()\n",
    "\n",
    "driver.find_element(By.XPATH,\"//span[text()='3-6 Lakhs']\").click()\n",
    "\n",
    "# Making empty list for our data to be dumped\n",
    "JobDesignation=[]\n",
    "JobLoc=[]\n",
    "CompanyName=[]\n",
    "Required_Exp=[]\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "# Extracting Data from The Website using XPATH and Dumping it in respective Lists\n",
    "jtt=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in jtt[0:10]:\n",
    "    title=i.text\n",
    "    JobDesignation.append(title)\n",
    "    \n",
    "jlt=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "for i in jlt[0:10]:\n",
    "    location=i.text\n",
    "    JobLoc.append(location)\n",
    "    \n",
    "cnt=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in cnt[0:10]:\n",
    "    compname=i.text\n",
    "    CompanyName.append(compname)\n",
    "    \n",
    "ret=driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]//span')\n",
    "for i in ret[0:10]:\n",
    "    exp=i.text\n",
    "    Required_Exp.append(exp)\n",
    "    \n",
    "\n",
    "# Using Pandas to Create DataFrame of the various Lists\n",
    "df=pd.DataFrame({'Job_Title':JobDesignation,'Job_Location':JobLoc,'Company_Name':CompanyName,'Experience_Requuired':Required_Exp})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676dc04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30132ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c54c3579",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "    1. Brand\n",
    "    2. Product Description\n",
    "    3. Price\n",
    "    The attributes which you have to scrape is ticked marked in the below image.\n",
    "    To scrape the data you have to go through following steps:\n",
    "        1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "        2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and\n",
    "            click the search icon\n",
    "        3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "            required data as usual.\n",
    "        4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "            click on it.\n",
    "        5. Now scrape data from this page as usual\n",
    "        6. Repeat this until you get data for 100 sunglasses.\n",
    "Note: That all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40a7c706",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_Discriptinon</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection, Gradient Rectangular Sunglasses...</td>\n",
       "      <td>₹359</td>\n",
       "      <td>81% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (Free Size)</td>\n",
       "      <td>₹599</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Retro Squ...</td>\n",
       "      <td>₹599</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Round Sunglasses (53)</td>\n",
       "      <td>₹749</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PHENOMENAL</td>\n",
       "      <td>UV Protection, Gradient Retro Square Sunglasse...</td>\n",
       "      <td>₹252</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection, Gradient Butterfly Sunglasses (62)</td>\n",
       "      <td>₹969</td>\n",
       "      <td>66% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Gradient, UV Protection Retro Square Sunglasse...</td>\n",
       "      <td>₹799</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>Polarized, UV Protection Wayfarer Sunglasses (61)</td>\n",
       "      <td>₹252</td>\n",
       "      <td>66% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Mirrored Aviator Sunglasses (55)</td>\n",
       "      <td>₹359</td>\n",
       "      <td>74% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>DAHAAZIL</td>\n",
       "      <td>UV Protection, Gradient Retro Square Sunglasse...</td>\n",
       "      <td>₹296</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Brand                               Product_Discriptinon Price  \\\n",
       "0     Fastrack  UV Protection, Gradient Rectangular Sunglasses...  ₹359   \n",
       "1     Fastrack  UV Protection Retro Square Sunglasses (Free Size)  ₹599   \n",
       "2     Fastrack  by Lenskart Polarized, UV Protection Retro Squ...  ₹599   \n",
       "3     Fastrack                UV Protection Round Sunglasses (53)  ₹749   \n",
       "4   PHENOMENAL  UV Protection, Gradient Retro Square Sunglasse...  ₹252   \n",
       "..         ...                                                ...   ...   \n",
       "95      PIRASO  UV Protection, Gradient Butterfly Sunglasses (62)  ₹969   \n",
       "96    Fastrack  Gradient, UV Protection Retro Square Sunglasse...  ₹799   \n",
       "97   New Specs  Polarized, UV Protection Wayfarer Sunglasses (61)  ₹252   \n",
       "98    Fastrack                   Mirrored Aviator Sunglasses (55)  ₹359   \n",
       "99    DAHAAZIL  UV Protection, Gradient Retro Square Sunglasse...  ₹296   \n",
       "\n",
       "   Discount  \n",
       "0   81% off  \n",
       "1   78% off  \n",
       "2   52% off  \n",
       "3   80% off  \n",
       "4   83% off  \n",
       "..      ...  \n",
       "95  66% off  \n",
       "96  87% off  \n",
       "97  66% off  \n",
       "98  74% off  \n",
       "99  70% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Creating an instance of webdriver for google chrome\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\RoshanV\\Desktop\\Internship\\Assignment 2\\chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "# Using webdriver we'll now open the flipkart website in chrome\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "driver.implicitly_wait(10)\n",
    "# Find using xpath\n",
    "close=driver.find_element(By.XPATH,'/html/body/div[2]/div/div/button').click()\n",
    "driver.implicitly_wait(10)\n",
    "# Find using xpath\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\").send_keys('sunglasses')\n",
    "click=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button').click()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "# Making empty list for our data to be dumped\n",
    "Brand=[]\n",
    "Product_Description=[]\n",
    "Price=[]\n",
    "Discount=[]\n",
    "\n",
    "\n",
    "# Extracting Data from The Website using XPATH and Dumping it in respective Lists\n",
    "\n",
    "#Parameter for looping onto multiple tabs to extract data\n",
    "start=0\n",
    "end=3\n",
    "\n",
    "for page in range(start,end):\n",
    "    com=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    for i in com:\n",
    "        Brand.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]//span').click()\n",
    "    time.sleep(5)\n",
    "\n",
    "for page in range(start,end):\n",
    "    dis=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    for i in dis:\n",
    "        Product_Description.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]//span').click()\n",
    "    time.sleep(5)\n",
    "\n",
    "for page in range(start,end):\n",
    "    value=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "    for i in value:\n",
    "        Price.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]//span').click()\n",
    "    time.sleep(5)    \n",
    "    \n",
    "    \n",
    "for page in range(start,end):\n",
    "    off=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]//span')\n",
    "    for i in off:\n",
    "        Discount.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]//span').click()\n",
    "    time.sleep(5)    \n",
    "    \n",
    "    \n",
    "# Using Pandas to Create DataFrame of the various Lists\n",
    "\n",
    "Data={'Brand':Brand[0:100],'Product_Discriptinon':Product_Description[0:100],'Price':Price[0:100],'Discount':Discount[0:100]}\n",
    "\n",
    "df=pd.DataFrame(Data)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca4efd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9614d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c847c37",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone.\n",
    "    This task will be done in following steps:\n",
    "    1. First get the webpage https://www.flipkart.com/\n",
    "    2. Enter “iphone 11” in “Search” field .\n",
    "    3. Then click the search button.\n",
    "        You will reach to the below shown webpage .\n",
    "        As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "        1. Rating\n",
    "        2. Review summary\n",
    "        3. Full review\n",
    "        4. You have to scrape this data for first 100 reviews.\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30b5ae05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Full_Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>What a camera .....just awesome ..you can feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Iphone is just awesome.. battery backup is ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific</td>\n",
       "      <td>Really worth of money. i just love it. It is t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>This is my first ever I phone. Before this I w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Best budget Iphone till date ❤️ go for it guys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Classy product</td>\n",
       "      <td>Totally in love with this ❤ the camera quality...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating               Review  \\\n",
       "0       5       Simply awesome   \n",
       "1       5     Perfect product!   \n",
       "2       5  Best in the market!   \n",
       "3       5   Highly recommended   \n",
       "4       5    Worth every penny   \n",
       "..    ...                  ...   \n",
       "95      5     Perfect product!   \n",
       "96      5             Terrific   \n",
       "97      5            Wonderful   \n",
       "98      5    Worth every penny   \n",
       "99      5       Classy product   \n",
       "\n",
       "                                         Full_Summary  \n",
       "0   Really satisfied with the Product I received.....  \n",
       "1   Amazing phone with great cameras and better ba...  \n",
       "2   Great iPhone very snappy experience as apple k...  \n",
       "3   What a camera .....just awesome ..you can feel...  \n",
       "4   Previously I was using one plus 3t it was a gr...  \n",
       "..                                                ...  \n",
       "95  Iphone is just awesome.. battery backup is ver...  \n",
       "96  Really worth of money. i just love it. It is t...  \n",
       "97  This is my first ever I phone. Before this I w...  \n",
       "98  Best budget Iphone till date ❤️ go for it guys...  \n",
       "99  Totally in love with this ❤ the camera quality...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Creating an instance of webdriver for google chrome\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\RoshanV\\Desktop\\Internship\\Assignment 2\\chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "# Using webdriver we'll now open the flipkart website in chrome\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "driver.implicitly_wait(10)\n",
    "# Find using xpath\n",
    "close=driver.find_element(By.XPATH,'/html/body/div[2]/div/div/button').click()\n",
    "driver.implicitly_wait(10)\n",
    "# Find using xpath\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\").send_keys('iphone')\n",
    "click=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button').click()\n",
    "time.sleep(10)\n",
    "# Find using xpath\n",
    "driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[2]/div/div/div/a/div[2]/div[1]/div[1]').click()\n",
    "time.sleep(10)\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-white-64-gb/p/itmfc6a7091eb20b?pid=MOBFWQ6BVWVEH3XE&lid=LSTMOBFWQ6BVWVEH3XEMXQMLO&marketplace=FLIPKART&q=iphone&store=search.flipkart.com%2Ftyy%2F4io&srno=s_1_1&otracker=search&otracker1=search&fm=organic&iid=8e57f974-8163-4000-ad39-c130783b0644.MOBFWQ6BVWVEH3XE.SEARCH&ppt=hp&ppn=homepage&ssid=vpzewiwbsw0000001660410983894&qH=0b3f45b266a97d70\")\n",
    "driver.implicitly_wait(10)\n",
    "# Find using xpath\n",
    "driver.find_element(By.XPATH,'//div[@class=\"_3UAT2v _16PBlm\"]//span').click()\n",
    "\n",
    "\n",
    "# Making empty list for our data to be dumped\n",
    "Rating=[]\n",
    "Review=[]\n",
    "Full_Summary=[]\n",
    "\n",
    "\n",
    "# Extracting Data from The Website using XPATH and Dumping it in respective Lists\n",
    "\n",
    "#Parameter for looping onto multiple tabs to extract data\n",
    "start=0\n",
    "end=10\n",
    "time.sleep(10)\n",
    "for page in range(start,end):\n",
    "    stars=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for i in stars:\n",
    "        Rating.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]//span').click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    \n",
    "for page in range(start,end):\n",
    "    summary=driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "    for i in summary:\n",
    "        Review.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]//span').click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    \n",
    "for page in range(start,end):\n",
    "    details=driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "    for i in details:\n",
    "        Full_Summary.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]//span').click()\n",
    "    time.sleep(5)\n",
    "  \n",
    "\n",
    " \n",
    "# Using Pandas to Create DataFrame of the various Lists\n",
    "    \n",
    "Data={\"Rating\":Rating,\"Review\":Review,\"Full_Summary\":Full_Summary}\n",
    "\n",
    "df=pd.DataFrame(Data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a609fa87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3a9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d71791fb",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "    search field.\n",
    "    You have to scrape 4 attributes of each sneaker:\n",
    "        1. Brand\n",
    "        2. Product Description\n",
    "        3. Price\n",
    "    As shown in the below image, you have to scrape the tick marked attributes.\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fd8a67a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_Discriptinon</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aadi</td>\n",
       "      <td>YODDHA High top elevated high Street Fashion S...</td>\n",
       "      <td>₹374</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KWIK FIT</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹397</td>\n",
       "      <td>72% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ONECENTRE</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹299</td>\n",
       "      <td>53% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Layasa</td>\n",
       "      <td>Puma Rebound LayUp SL Sneakers For Men</td>\n",
       "      <td>₹228</td>\n",
       "      <td>48% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deals4you</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹394</td>\n",
       "      <td>54% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Series 7 Sneakers For Men</td>\n",
       "      <td>₹319</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ASTEROID</td>\n",
       "      <td>411 Casual Sneakers For Men</td>\n",
       "      <td>₹499</td>\n",
       "      <td>67% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>RED TAPE</td>\n",
       "      <td>Sneakers For Women</td>\n",
       "      <td>₹1,299</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Creer</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹298</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Layasa</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹199</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Brand                               Product_Discriptinon   Price  \\\n",
       "0        aadi  YODDHA High top elevated high Street Fashion S...    ₹374   \n",
       "1    KWIK FIT                                   Sneakers For Men    ₹397   \n",
       "2   ONECENTRE                                   Sneakers For Men    ₹299   \n",
       "3      Layasa             Puma Rebound LayUp SL Sneakers For Men    ₹228   \n",
       "4   Deals4you                                   Sneakers For Men    ₹394   \n",
       "..        ...                                                ...     ...   \n",
       "95     BRUTON                          Series 7 Sneakers For Men    ₹319   \n",
       "96   ASTEROID                        411 Casual Sneakers For Men    ₹499   \n",
       "97   RED TAPE                                 Sneakers For Women  ₹1,299   \n",
       "98      Creer                                   Sneakers For Men    ₹298   \n",
       "99     Layasa                                   Sneakers For Men    ₹199   \n",
       "\n",
       "   Discount  \n",
       "0   50% off  \n",
       "1   72% off  \n",
       "2   53% off  \n",
       "3   48% off  \n",
       "4   54% off  \n",
       "..      ...  \n",
       "95  60% off  \n",
       "96  67% off  \n",
       "97  52% off  \n",
       "98  60% off  \n",
       "99  62% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Creating an instance of webdriver for google chrome\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\RoshanV\\Desktop\\Internship\\Assignment 2\\chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "# Using webdriver we'll now open the flipkart website in chrome\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "driver.implicitly_wait(10)\n",
    "# Find using xpath\n",
    "close=driver.find_element(By.XPATH,'/html/body/div[2]/div/div/button').click()\n",
    "driver.implicitly_wait(10)\n",
    "# Find using xpath\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\").send_keys('sneakers')\n",
    "click=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button').click()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "# Making empty list for our data to be dumped\n",
    "Brand=[]\n",
    "Product_Description=[]\n",
    "Price=[]\n",
    "Discount=[]\n",
    "\n",
    "\n",
    "# Extracting Data from The Website using XPATH and Dumping it in respective Lists\n",
    "\n",
    "#Parameter for looping onto multiple tabs to extract data\n",
    "start=0\n",
    "end=3\n",
    "\n",
    "for page in range(start,end):\n",
    "    com=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    for i in com:\n",
    "        Brand.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]//span').click()\n",
    "    time.sleep(5)\n",
    "\n",
    "for page in range(start,end):\n",
    "    dis=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    for i in dis:\n",
    "        Product_Description.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]//span').click()\n",
    "    time.sleep(5)\n",
    "\n",
    "for page in range(start,end):\n",
    "    value=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "    for i in value:\n",
    "        Price.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]//span').click()\n",
    "    time.sleep(5)    \n",
    "    \n",
    "    \n",
    "for page in range(start,end):\n",
    "    off=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]//span')\n",
    "    for i in off:\n",
    "        Discount.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]//span').click()\n",
    "    time.sleep(5)    \n",
    "    \n",
    "    \n",
    "# Using Pandas to Create DataFrame of the various Lists    \n",
    "Data={'Brand':Brand[0:100],'Product_Discriptinon':Product_Description[0:100],'Price':Price[0:100],'Discount':Discount[0:100]}\n",
    "\n",
    "df=pd.DataFrame(Data)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023999a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9333b1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82b8f219",
   "metadata": {},
   "source": [
    "Q7: Go to the link - https://www.myntra.com/shoes\n",
    "    Set second Price filter and Color filter to “Black”, as shown in the below image.\n",
    "    And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe\n",
    "    description, price of the shoe as shown in the below image.\n",
    "\n",
    "Note: Applying the filter and scraping the data, everything should be done through code only and there\n",
    "        should not be any manual step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb85ca2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Discription</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skechers</td>\n",
       "      <td>Cabo WP Riding Boots</td>\n",
       "      <td>Rs. 8449Rs. 12999(35% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADIDAS Originals</td>\n",
       "      <td>Women Leather Heeled Mules</td>\n",
       "      <td>Rs. 6999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Leather Block Sandals</td>\n",
       "      <td>Rs. 6990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Evoride 3 Running Shoes</td>\n",
       "      <td>Rs. 11999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men Leather Sneakers</td>\n",
       "      <td>Rs. 11990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>High-Top Platform Heeled Boots</td>\n",
       "      <td>Rs. 8119Rs. 13999(42% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>Women High-Top Heeled Boots</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Heavenly Omni-Heat</td>\n",
       "      <td>Rs. 7495Rs. 14990(50% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Leather Metal Chain Boots</td>\n",
       "      <td>Rs. 8399Rs. 13999(40% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Birkenstock</td>\n",
       "      <td>Unisex Supernova Cushion 7</td>\n",
       "      <td>Rs. 11990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Brand                     Discription  \\\n",
       "0           Skechers            Cabo WP Riding Boots   \n",
       "1   ADIDAS Originals      Women Leather Heeled Mules   \n",
       "2               Nike           Leather Block Sandals   \n",
       "3               Nike         Evoride 3 Running Shoes   \n",
       "4               Nike            Men Leather Sneakers   \n",
       "..               ...                             ...   \n",
       "95            ADIDAS  High-Top Platform Heeled Boots   \n",
       "96            ADIDAS     Women High-Top Heeled Boots   \n",
       "97              Puma              Heavenly Omni-Heat   \n",
       "98              ALDO       Leather Metal Chain Boots   \n",
       "99       Birkenstock      Unisex Supernova Cushion 7   \n",
       "\n",
       "                         Price  \n",
       "0   Rs. 8449Rs. 12999(35% OFF)  \n",
       "1                     Rs. 6999  \n",
       "2                     Rs. 6990  \n",
       "3                    Rs. 11999  \n",
       "4                    Rs. 11990  \n",
       "..                         ...  \n",
       "95  Rs. 8119Rs. 13999(42% OFF)  \n",
       "96                    Rs. 7999  \n",
       "97  Rs. 7495Rs. 14990(50% OFF)  \n",
       "98  Rs. 8399Rs. 13999(40% OFF)  \n",
       "99                   Rs. 11990  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Creating an instance of webdriver for google chrome\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\RoshanV\\Desktop\\Internship\\Assignment 2\\chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "# Using webdriver we'll now open the website in chrome\n",
    "driver.get(\"http://www.myntra.com/shoes\")\n",
    "# Find using xpath\n",
    "driver.find_element(By.XPATH,'/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label').click()\n",
    "time.sleep(5)\n",
    "# Find using xpath\n",
    "driver.find_element(By.XPATH,'/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label').click()\n",
    "\n",
    "# Making empty list for our data to be dumped\n",
    "Brand=[]\n",
    "Discription=[]\n",
    "Price=[]\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "# Extracting Data from The Website using XPATH and Dumping it in respective Lists\n",
    "\n",
    "#Parameter for looping onto multiple tabs to extract data\n",
    "start=0\n",
    "end=2\n",
    "\n",
    "\n",
    "for page in range(start,end):\n",
    "    com=driver.find_elements(By.XPATH,'//h3[@class=\"product-brand\"]')\n",
    "    for i in com:\n",
    "        Brand.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//li[@class=\"pagination-next\"]//a').click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "\n",
    "\n",
    "for page in range(start,end):\n",
    "    detail=driver.find_elements(By.XPATH,'//h4[@class=\"product-product\"]')\n",
    "    for i in detail:\n",
    "        Discription.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//li[@class=\"pagination-next\"]//a').click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    \n",
    "\n",
    "for page in range(start,end):\n",
    "    rate=driver.find_elements(By.XPATH,'//div[@class=\"product-price\"]')\n",
    "    for i in rate:\n",
    "        Price.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//li[@class=\"pagination-next\"]//a').click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Using Pandas to Create DataFrame of the various Lists    \n",
    "Data={'Brand':Brand,'Discription':Discription,'Price':Price}\n",
    "\n",
    "df=pd.DataFrame(Data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936b8da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453aa112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f5786e6",
   "metadata": {},
   "source": [
    "Q8: Go to webpage https://www.amazon.in/\n",
    "    Enter “Laptop” in the search field and then click the search icon.\n",
    "    Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "    After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "        1. Title\n",
    "        2. Ratings\n",
    "        3. Price\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56a412b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lenovo Ideapad Gaming 3 Intel Core i7 10th Gen...</td>\n",
       "      <td>22</td>\n",
       "      <td>59,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samsung Galaxy Book2 Intel 12th Gen core i7 39...</td>\n",
       "      <td>25</td>\n",
       "      <td>79,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASUS Vivobook 15, 15.6-inch (39.62 cms) FHD, I...</td>\n",
       "      <td>22</td>\n",
       "      <td>54,845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HP Pavilion Plus, 12th Gen Intel Core i7 16GB ...</td>\n",
       "      <td>16</td>\n",
       "      <td>89,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...</td>\n",
       "      <td>15</td>\n",
       "      <td>87,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hp Pavilion X360 11Th Gen Intel Core I7 14 Inc...</td>\n",
       "      <td>22</td>\n",
       "      <td>82,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lenovo Yoga 7 Intel Evo Core i7 12th Gen 14\"(3...</td>\n",
       "      <td>7</td>\n",
       "      <td>1,09,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Acer Nitro 5 Gaming Laptop/12th Gen Intel Core...</td>\n",
       "      <td>23</td>\n",
       "      <td>1,04,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ASUS TUF Gaming F15 (2022), 15.6-inch (39.62 c...</td>\n",
       "      <td>1</td>\n",
       "      <td>1,07,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lenovo IdeaPad Slim 5 Intel Core i7 12th Gen 1...</td>\n",
       "      <td>15</td>\n",
       "      <td>81,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title Rating     Price\n",
       "0  Lenovo Ideapad Gaming 3 Intel Core i7 10th Gen...     22    59,990\n",
       "1  Samsung Galaxy Book2 Intel 12th Gen core i7 39...     25    79,490\n",
       "2  ASUS Vivobook 15, 15.6-inch (39.62 cms) FHD, I...     22    54,845\n",
       "3  HP Pavilion Plus, 12th Gen Intel Core i7 16GB ...     16    89,990\n",
       "4  HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...     15    87,990\n",
       "5  Hp Pavilion X360 11Th Gen Intel Core I7 14 Inc...     22    82,400\n",
       "6  Lenovo Yoga 7 Intel Evo Core i7 12th Gen 14\"(3...      7  1,09,990\n",
       "7  Acer Nitro 5 Gaming Laptop/12th Gen Intel Core...     23  1,04,990\n",
       "8  ASUS TUF Gaming F15 (2022), 15.6-inch (39.62 c...      1  1,07,990\n",
       "9  Lenovo IdeaPad Slim 5 Intel Core i7 12th Gen 1...     15    81,990"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Creating an instance of webdriver for google chrome\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\RoshanV\\Desktop\\Internship\\Assignment 2\\chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "# Using webdriver we'll now open the amazon website in chrome\n",
    "driver.get(\"http://www.amazon.in\")\n",
    "driver.implicitly_wait(10)\n",
    "# Find using xpath\n",
    "driver.find_element(By.XPATH,'//div[@class=\"nav-search-field \"]//input').send_keys(\"Laptop\")\n",
    "time.sleep(5)\n",
    "# Find using xpath\n",
    "driver.find_element(By.XPATH,'//div[@class=\"nav-search-submit nav-sprite\"]//span').click()\n",
    "# Find using xpath\n",
    "driver.find_element(By.XPATH,'//li[@id=\"p_n_feature_thirteen_browse-bin/12598163031\"]//span').click()\n",
    "\n",
    "\n",
    "# Making empty list for our data to be dumped\n",
    "Title=[]\n",
    "Rating=[]\n",
    "Price=[]\n",
    "\n",
    "\n",
    "# Extracting Data from The Website using XPATH and Dumping it in respective Lists\n",
    "model=driver.find_elements(By.XPATH,'//div[@class=\"a-section a-spacing-none puis-padding-right-small s-title-instructions-style\"]//h2')\n",
    "for i in model[0:10]:\n",
    "    Title.append(i.text)\n",
    "    \n",
    "    \n",
    "rate=driver.find_elements(By.XPATH,'//div[@class=\"a-row a-size-small\"]')\n",
    "for i in rate[0:10]:\n",
    "    Rating.append(i.text)\n",
    "    \n",
    "    \n",
    "cost=driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "for i in cost[0:10]:\n",
    "    Price.append(i.text)\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    " # Using Pandas to Create DataFrame of the various Lists\n",
    "Data={\"Title\":Title,\"Rating\":Rating,\"Price\":Price}\n",
    "\n",
    "df=pd.DataFrame(Data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28181e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e997c146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcd936e8",
   "metadata": {},
   "source": [
    "Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida\n",
    "    location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "    This task will be done in following steps:\n",
    "        1. First get the webpage https://www.ambitionbox.com/\n",
    "        2. Click on the Job option as shown in the image\n",
    "        3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter\n",
    "            “Data Scientist” and click on search button.\n",
    "        4. You will reach to the following web page click on location and in place of “Search location” enter\n",
    "            “Noida” and select location “Noida”.\n",
    "        5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "        6. Finally create a dataframe of the scraped data.\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cd3de6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Designation</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Locations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hiring Data Scientist #productbasecompany #CBRE</td>\n",
       "      <td>CBRE South Asia Pvt Ltd</td>\n",
       "      <td>Hyderabad/Secunderabad, Gurgaon/Gurugram, Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Ericsson India Global Services Pvt. Ltd.</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>Ghaziabad, Gurgaon/Gurugram, Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Optum Global Solutions (India) Private Limited</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Associate Manager - Data Scientist</td>\n",
       "      <td>Optum Global Solutions (India) Private Limited</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Ericsson India Global Services Pvt. Ltd.</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>SOPRA STERIA INDIA LIMITED</td>\n",
       "      <td>Chennai, Bengaluru/Bangalore, Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist - Noida/Bangalore</td>\n",
       "      <td>EXL Services.com ( I ) Pvt. Ltd.</td>\n",
       "      <td>Bengaluru/Bangalore, Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EY - Data Engineer - Data Integration/Modeling...</td>\n",
       "      <td>EY</td>\n",
       "      <td>Pune, Bengaluru/Bangalore, Hyderabad/Secundera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist Intern</td>\n",
       "      <td>CARE HEALTH INSURANCE LIMITED</td>\n",
       "      <td>Faridabad, Delhi NCR, Gurgaon/Gurugram +1 more</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Designation  \\\n",
       "0    Hiring Data Scientist #productbasecompany #CBRE   \n",
       "1                                     Data Scientist   \n",
       "2                                     Data Scientist   \n",
       "3                                     Data Scientist   \n",
       "4                 Associate Manager - Data Scientist   \n",
       "5                                     Data Scientist   \n",
       "6                                     Data Scientist   \n",
       "7                   Data Scientist - Noida/Bangalore   \n",
       "8  EY - Data Engineer - Data Integration/Modeling...   \n",
       "9                              Data Scientist Intern   \n",
       "\n",
       "                                     Company_Name  \\\n",
       "0                         CBRE South Asia Pvt Ltd   \n",
       "1        Ericsson India Global Services Pvt. Ltd.   \n",
       "2                   GENPACT India Private Limited   \n",
       "3  Optum Global Solutions (India) Private Limited   \n",
       "4  Optum Global Solutions (India) Private Limited   \n",
       "5        Ericsson India Global Services Pvt. Ltd.   \n",
       "6                      SOPRA STERIA INDIA LIMITED   \n",
       "7                EXL Services.com ( I ) Pvt. Ltd.   \n",
       "8                                              EY   \n",
       "9                   CARE HEALTH INSURANCE LIMITED   \n",
       "\n",
       "                                           Locations  \n",
       "0    Hyderabad/Secunderabad, Gurgaon/Gurugram, Noida  \n",
       "1                                              Noida  \n",
       "2                 Ghaziabad, Gurgaon/Gurugram, Noida  \n",
       "3                                              Noida  \n",
       "4                                              Noida  \n",
       "5                                              Noida  \n",
       "6                Chennai, Bengaluru/Bangalore, Noida  \n",
       "7                         Bengaluru/Bangalore, Noida  \n",
       "8  Pune, Bengaluru/Bangalore, Hyderabad/Secundera...  \n",
       "9     Faridabad, Delhi NCR, Gurgaon/Gurugram +1 more  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Creating an instance of webdriver for google chrome\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\RoshanV\\Desktop\\Internship\\Assignment 2\\chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "# Using webdriver we'll now open the website in chrome\n",
    "driver.get(\"http://www.ambitionbox.com\")\n",
    "driver.implicitly_wait(10)\n",
    "# Find using xpath\n",
    "driver.find_element(By.XPATH,'/html/body/div[1]/nav[2]/div/ul/li[5]/a').click()\n",
    "# Find using xpath\n",
    "driver.find_element(By.XPATH,'//div[@class=\"search-company\"]//span//input').send_keys('Data Scientist')\n",
    "driver.find_element(By.XPATH,'/html/body/div[1]/div/div/div[2]/div[1]/div[1]/div/div/div/button/span').click()\n",
    "# Find using xpath\n",
    "driver.find_element(By.XPATH,'/html/body/div[1]/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[1]/p').click()\n",
    "time.sleep(5)\n",
    "# Find using xpath\n",
    "driver.find_element(By.XPATH,'/html/body/div[1]/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[2]/input').click()\n",
    "driver.find_element(By.XPATH,'/html/body/div[1]/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[2]/input').send_keys('Noida')\n",
    "time.sleep(5)\n",
    "# Find using xpath\n",
    "driver.find_element(By.XPATH,'/html/body/div[1]/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[3]/div[1]/div[1]/div/label').click()\n",
    "\n",
    "\n",
    "# Making empty list for our data to be dumped\n",
    "time.sleep(5)\n",
    "Position=[]\n",
    "Company_Name=[]\n",
    "Location=[]\n",
    "\n",
    "\n",
    "# Extracting Data from The Website using XPATH and Dumping it in respective Lists\n",
    "model=driver.find_elements(By.XPATH,'//a[@class=\"title noclick\"]')\n",
    "for i in model[0:10]:\n",
    "    Position.append(i.text)\n",
    "    \n",
    "time.sleep(5)\n",
    "model=driver.find_elements(By.XPATH,'//p[@class=\"company body-medium\"]')\n",
    "for i in model[0:10]:\n",
    "    Company_Name.append(i.text)\n",
    "       \n",
    "\n",
    "time.sleep(5)\n",
    "model=driver.find_elements(By.XPATH,'//div[@class=\"entity loc\"]//p')\n",
    "for i in model[1:11]:\n",
    "    Location.append(i.text)\n",
    "    \n",
    "\n",
    "    \n",
    "# Using Pandas to Create DataFrame of the various Lists    \n",
    "Data={\"Designation\":Position,\"Company_Name\":Company_Name,\"Locations\":Location}\n",
    "\n",
    "df=pd.DataFrame(Data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a02a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3377c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26dfe40c",
   "metadata": {},
   "source": [
    "Q10: Write a python program to scrape the salary data for Data Scientist designation.\n",
    "    You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary.\n",
    "    The above task will be, done as shown in the below steps:\n",
    "        1. First get the webpage https://www.ambitionbox.com/\n",
    "        2. Click on the salaries option as shown in the image.\n",
    "        3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and\n",
    "            then click on “Data Scientist”.\n",
    "            You have to scrape the data ticked in the above image.\n",
    "        4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average\n",
    "            salary, minimum salary, maximum salary, experience required.\n",
    "        5. Store the data in a dataframe.\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cb327c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Min_Salary</th>\n",
       "      <th>Max_Salary</th>\n",
       "      <th>Avg_Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart\\nData Scientist Salary</td>\n",
       "      <td>3-4 yrs experience (based on 22 salaries)</td>\n",
       "      <td>₹ 25.0L</td>\n",
       "      <td>₹ 45.0L</td>\n",
       "      <td>₹ 31.7L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ab Inbev\\nData Scientist Salary</td>\n",
       "      <td>2-4 yrs experience (based on 53 salaries)</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>₹ 25.5L</td>\n",
       "      <td>₹ 19.7L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Optum\\nData Scientist Salary</td>\n",
       "      <td>2-4 yrs experience (based on 48 salaries)</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 22.6L</td>\n",
       "      <td>₹ 16.5L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZS\\nData Scientist Salary</td>\n",
       "      <td>1-2 yrs experience (based on 33 salaries)</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 22.0L</td>\n",
       "      <td>₹ 15.7L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fractal Analytics\\nData Scientist Salary</td>\n",
       "      <td>2-4 yrs experience (based on 109 salaries)</td>\n",
       "      <td>₹ 9.0L</td>\n",
       "      <td>₹ 23.0L</td>\n",
       "      <td>₹ 15.2L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tiger Analytics\\nData Scientist Salary</td>\n",
       "      <td>2-4 yrs experience (based on 65 salaries)</td>\n",
       "      <td>₹ 9.0L</td>\n",
       "      <td>₹ 20.0L</td>\n",
       "      <td>₹ 14.7L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Legato Health Technologies\\nData Scientist Salary</td>\n",
       "      <td>4 yrs experience (based on 11 salaries)</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 20.0L</td>\n",
       "      <td>₹ 14.5L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tredence\\nData Scientist Salary</td>\n",
       "      <td>3 yrs experience (based on 12 salaries)</td>\n",
       "      <td>₹ 8.8L</td>\n",
       "      <td>₹ 17.5L</td>\n",
       "      <td>₹ 14.1L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UnitedHealth\\nData Scientist Salary</td>\n",
       "      <td>2-4 yrs experience (based on 91 salaries)</td>\n",
       "      <td>₹ 8.0L</td>\n",
       "      <td>₹ 20.5L</td>\n",
       "      <td>₹ 13.6L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ford Motor\\nData Scientist Salary</td>\n",
       "      <td>3-4 yrs experience (based on 21 salaries)</td>\n",
       "      <td>₹ 10.0L</td>\n",
       "      <td>₹ 18.0L</td>\n",
       "      <td>₹ 13.5L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Company_Name  \\\n",
       "0                     Walmart\\nData Scientist Salary   \n",
       "1                    Ab Inbev\\nData Scientist Salary   \n",
       "2                       Optum\\nData Scientist Salary   \n",
       "3                          ZS\\nData Scientist Salary   \n",
       "4           Fractal Analytics\\nData Scientist Salary   \n",
       "5             Tiger Analytics\\nData Scientist Salary   \n",
       "6  Legato Health Technologies\\nData Scientist Salary   \n",
       "7                    Tredence\\nData Scientist Salary   \n",
       "8                UnitedHealth\\nData Scientist Salary   \n",
       "9                  Ford Motor\\nData Scientist Salary   \n",
       "\n",
       "                                   Experience Min_Salary Max_Salary Avg_Salary  \n",
       "0   3-4 yrs experience (based on 22 salaries)    ₹ 25.0L    ₹ 45.0L    ₹ 31.7L  \n",
       "1   2-4 yrs experience (based on 53 salaries)    ₹ 15.0L    ₹ 25.5L    ₹ 19.7L  \n",
       "2   2-4 yrs experience (based on 48 salaries)    ₹ 11.0L    ₹ 22.6L    ₹ 16.5L  \n",
       "3   1-2 yrs experience (based on 33 salaries)    ₹ 11.0L    ₹ 22.0L    ₹ 15.7L  \n",
       "4  2-4 yrs experience (based on 109 salaries)     ₹ 9.0L    ₹ 23.0L    ₹ 15.2L  \n",
       "5   2-4 yrs experience (based on 65 salaries)     ₹ 9.0L    ₹ 20.0L    ₹ 14.7L  \n",
       "6     4 yrs experience (based on 11 salaries)    ₹ 11.0L    ₹ 20.0L    ₹ 14.5L  \n",
       "7     3 yrs experience (based on 12 salaries)     ₹ 8.8L    ₹ 17.5L    ₹ 14.1L  \n",
       "8   2-4 yrs experience (based on 91 salaries)     ₹ 8.0L    ₹ 20.5L    ₹ 13.6L  \n",
       "9   3-4 yrs experience (based on 21 salaries)    ₹ 10.0L    ₹ 18.0L    ₹ 13.5L  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Creating an instance of webdriver for google chrome\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\RoshanV\\Desktop\\Internship\\Assignment 2\\chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "# Using webdriver we'll now open the website in chrome\n",
    "driver.get(\"http://www.ambitionbox.com\")\n",
    "driver.implicitly_wait(10)\n",
    "# Find using xpath\n",
    "driver.find_element(By.XPATH,'/html/body/section[3]/div/div[1]/a').click()\n",
    "\n",
    "# Find using xpath\n",
    "driver.find_element(By.XPATH,'/html/body/div[1]/div/div/main/section[1]/div[2]/div[1]/span/input').send_keys('Data Scientist')\n",
    "driver.find_element(By.XPATH,'/html/body/div[1]/div/div/main/section[1]/div[2]/div[1]/span/div/div/div[1]/div/div/p').click()\n",
    "\n",
    "\n",
    "# Making empty list for our data to be dumped\n",
    "time.sleep(5)\n",
    "Experience=[]\n",
    "Company_Name=[]\n",
    "Min_Salary=[]\n",
    "Max_Salary=[]\n",
    "Avg_Salary=[]\n",
    "\n",
    "\n",
    "# Extracting Data from The Website using XPATH and Dumping it in respective Lists\n",
    "exp=driver.find_elements(By.XPATH,'//div[@class=\"sbold-list-header\"]')\n",
    "for i in exp:\n",
    "    Experience.append(i.text)\n",
    "    \n",
    "time.sleep(5)\n",
    "model=driver.find_elements(By.XPATH,'//div[@class=\"company-info\"]//a')\n",
    "for i in model:\n",
    "    Company_Name.append(i.text)\n",
    "       \n",
    "\n",
    "time.sleep(5)\n",
    "min=driver.find_elements(By.XPATH,'//div[@class=\"value body-medium\"]')\n",
    "for i in min[0::2]:\n",
    "    Min_Salary.append(i.text)\n",
    "\n",
    "max=driver.find_elements(By.XPATH,'//div[@class=\"value body-medium\"]')\n",
    "for i in max[1::2]:\n",
    "    Max_Salary.append(i.text)\n",
    "    \n",
    "time.sleep(5)\n",
    "avg=driver.find_elements(By.XPATH,'//div[@class=\"average-indicator-wrapper\"]//p')\n",
    "for i in avg:\n",
    "    Avg_Salary.append(i.text)\n",
    "       \n",
    "\n",
    "# Using Pandas to Create DataFrame of the various Lists\n",
    "Data={\"Company_Name\":Company_Name,\"Experience\":Experience,\"Min_Salary\":Min_Salary,\"Max_Salary\":Max_Salary,\"Avg_Salary\":Avg_Salary}\n",
    "\n",
    "df=pd.DataFrame(Data)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
